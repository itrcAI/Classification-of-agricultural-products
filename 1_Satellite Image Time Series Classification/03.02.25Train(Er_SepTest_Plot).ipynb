{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "metadata": {}
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mhbokaei/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torchnet as tnt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import json\n",
        "import pickle as pkl\n",
        "import argparse\n",
        "import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'1.12.1+cu116'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.__version__\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vUEHAOQlsCLH"
      },
      "outputs": [],
      "source": [
        "from models.stclassifier import PseTae\n",
        "from dataset import PixelSetData, PixelSetData_preloaded\n",
        "from learning.focal_loss import FocalLoss\n",
        "from learning.weight_init import weight_init\n",
        "from learning.metrics import mIou, confusion_matrix_analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0a05UEVqsCNm"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_epoch(model, optimizer, criterion, data_loader, device, args):\n",
        "    acc_meter = tnt.meter.ClassErrorMeter(accuracy=True)\n",
        "    loss_meter = tnt.meter.AverageValueMeter()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for i, (x, y) in enumerate(data_loader):\n",
        "\n",
        "        y_true.extend(list(map(int, y)))\n",
        "\n",
        "        x = recursive_todevice(x, device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = out.detach()\n",
        "        y_p = pred.argmax(dim=1).cpu().numpy()\n",
        "        y_pred.extend(list(y_p))\n",
        "        acc_meter.add(pred, y)\n",
        "        loss_meter.add(loss.item())\n",
        "\n",
        "        if (i + 1) % args['display_step'] == 0:\n",
        "            print('Step [{}/{}], Loss: {:.4f}, Acc : {:.2f}'.format(i + 1, len(data_loader), loss_meter.value()[0],\n",
        "                                                                    acc_meter.value()[0]))\n",
        "\n",
        "    epoch_metrics = {'train_loss': loss_meter.value()[0],\n",
        "                     'train_accuracy': acc_meter.value()[0],\n",
        "                     'train_IoU': mIou(y_true, y_pred, n_classes=args['num_classes'])}\n",
        "\n",
        "    return epoch_metrics\n",
        "\n",
        "\n",
        "def evaluation(model, criterion, loader, device, args, mode='val'):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    acc_meter = tnt.meter.ClassErrorMeter(accuracy=True)\n",
        "    loss_meter = tnt.meter.AverageValueMeter()\n",
        "\n",
        "    for (x, y) in loader:\n",
        "        y_true.extend(list(map(int, y)))\n",
        "        x = recursive_todevice(x, device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            prediction = model(x)\n",
        "            loss = criterion(prediction, y)\n",
        "\n",
        "        acc_meter.add(prediction, y)\n",
        "        loss_meter.add(loss.item())\n",
        "\n",
        "        y_p = prediction.argmax(dim=1).cpu().numpy()\n",
        "        y_pred.extend(list(y_p))\n",
        "\n",
        "    metrics = {'{}_accuracy'.format(mode): acc_meter.value()[0],\n",
        "               '{}_loss'.format(mode): loss_meter.value()[0],\n",
        "               '{}_IoU'.format(mode): mIou(y_true, y_pred, args['num_classes'])}\n",
        "\n",
        "    if mode == 'val':\n",
        "        return metrics\n",
        "    elif mode == 'test':\n",
        "        return metrics, confusion_matrix(y_true, y_pred, labels=list(range(args['num_classes'])))\n",
        "\n",
        "\n",
        "def get_loaders(dt, kfold, args):\n",
        "    indices = list(range(len(dt)))\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    kf = KFold(n_splits=kfold, shuffle=False)\n",
        "    indices_seq = list(kf.split(list(range(len(dt)))))\n",
        "    ntest = len(indices_seq[0][1])\n",
        "\n",
        "    loader_seq = []\n",
        "    for trainval, test_indices in indices_seq:\n",
        "        trainval = [indices[i] for i in trainval]\n",
        "        test_indices = [indices[i] for i in test_indices]\n",
        "\n",
        "        validation_indices = trainval[-ntest:]\n",
        "        train_indices = trainval[:-ntest]\n",
        "\n",
        "        train_sampler = data.sampler.SubsetRandomSampler(train_indices)\n",
        "        validation_sampler = data.sampler.SubsetRandomSampler(validation_indices)\n",
        "        test_sampler = data.sampler.SubsetRandomSampler(test_indices)\n",
        "\n",
        "        train_loader = data.DataLoader(dt, batch_size=args['batch_size'],\n",
        "                                       sampler=train_sampler,\n",
        "                                       num_workers=args['num_workers'])\n",
        "        validation_loader = data.DataLoader(dt, batch_size=args['batch_size'],\n",
        "                                            sampler=validation_sampler,\n",
        "                                            num_workers=args['num_workers'])\n",
        "        test_loader = data.DataLoader(dt, batch_size=args['batch_size'],\n",
        "                                      sampler=test_sampler,\n",
        "                                      num_workers=args['num_workers'])\n",
        "\n",
        "        loader_seq.append((train_loader, validation_loader, test_loader))\n",
        "    return loader_seq\n",
        "\n",
        "\n",
        "def get_loaders_sepT(dt,dt_sepT, kfold, args):\n",
        "    indices = list(range(len(dt)))\n",
        "    indices_T = list(range(len(dt_sepT)))\n",
        "    np.random.shuffle(indices)\n",
        "    np.random.shuffle(indices_T)\n",
        "\n",
        "\n",
        "    kf = KFold(n_splits=kfold, shuffle=False)\n",
        "    indices_seq = list(kf.split(list(range(len(dt)))))\n",
        "    indices_seq_T = list(range(len(dt_sepT)))\n",
        "\n",
        "    def add_element(main_matrix, \n",
        "                    temp_matrix = indices_seq_T):\n",
        "        main_matrix = list(main_matrix)\n",
        "        main_matrix.append(temp_matrix)\n",
        "        main_matrix = tuple(main_matrix)\n",
        "        return main_matrix\n",
        "\n",
        "    indices_seqq = tuple(map(add_element , indices_seq))\n",
        "\n",
        "    loader_seq = []\n",
        "    for train_indices, validation_indices, test_indices in indices_seqq:\n",
        "        train_indices = [indices[i] for i in train_indices]\n",
        "        validation_indices = [indices[i] for i in validation_indices]\n",
        "        test_indices = [indices_T[i] for i in test_indices]\n",
        "        \n",
        "        train_sampler = data.sampler.SubsetRandomSampler(train_indices)\n",
        "        validation_sampler = data.sampler.SubsetRandomSampler(validation_indices)\n",
        "        test_sampler = data.sampler.SubsetRandomSampler(test_indices)\n",
        "\n",
        "        train_loader = data.DataLoader(dt, batch_size=args['batch_size'],\n",
        "                                       sampler=train_sampler,\n",
        "                                       num_workers=args['num_workers'])\n",
        "        validation_loader = data.DataLoader(dt, batch_size=args['batch_size'],\n",
        "                                            sampler=validation_sampler,\n",
        "                                            num_workers=args['num_workers'])\n",
        "        test_loader = data.DataLoader(dt_sepT, batch_size=args['batch_size'],\n",
        "                                      sampler=test_sampler,\n",
        "                                      num_workers=args['num_workers'])\n",
        "\n",
        "        loader_seq.append((train_loader, validation_loader, test_loader))\n",
        "    return loader_seq\n",
        "\n",
        "def recursive_todevice(x, device):\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return x.to(device)\n",
        "    else:\n",
        "        return [recursive_todevice(c, device) for c in x]\n",
        "\n",
        "\n",
        "def prepare_output(args):\n",
        "    os.makedirs(args['res_dir'], exist_ok=True)\n",
        "    for fold in range(1, args['kfold'] + 1):\n",
        "        os.makedirs(os.path.join(args['res_dir'], 'Fold_{}'.format(fold)), exist_ok=True)\n",
        "\n",
        "\n",
        "def checkpoint(fold, log, args):\n",
        "    with open(os.path.join(args['res_dir'], 'Fold_{}'.format(fold), 'trainlog.json'), 'w') as outfile:\n",
        "        json.dump(log, outfile, indent=4)\n",
        "\n",
        "\n",
        "def save_results(fold, metrics, conf_mat, args):\n",
        "    with open(os.path.join(args['res_dir'], 'Fold_{}'.format(fold), 'test_metrics.json'), 'w') as outfile:\n",
        "        json.dump(metrics, outfile, indent=4)\n",
        "    pkl.dump(conf_mat, open(os.path.join(args['res_dir'], 'Fold_{}'.format(fold), 'conf_mat.pkl'), 'wb'))\n",
        "\n",
        "\n",
        "def overall_performance(args):\n",
        "    cm = np.zeros((args['num_classes'], args['num_classes']))\n",
        "    for fold in range(1, args['kfold'] + 1):\n",
        "        cm += pkl.load(open(os.path.join(args['res_dir'], 'Fold_{}'.format(fold), 'conf_mat.pkl'), 'rb'))\n",
        "\n",
        "    _, perf = confusion_matrix_analysis(cm)\n",
        "\n",
        "    print('Overall performance:')\n",
        "    print('Acc: {},  IoU: {}'.format(perf['Accuracy'], perf['MACRO_IoU']))\n",
        "\n",
        "    with open(os.path.join(args['res_dir'], 'overall.json'), 'w') as file:\n",
        "        file.write(json.dumps(perf, indent=4))\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    np.random.seed(args['rdm_seed'])\n",
        "    torch.manual_seed(args['rdm_seed'])\n",
        "    prepare_output(args)\n",
        "    mean_std = pkl.load(open(args['dataset_folder'] + '/S2-2017-T31TFM-meanstd.pkl', 'rb'))\n",
        "    mean_std_sepT = pkl.load(open(args['dataset_folder_sepT'] + '/S2-2017-T31TFM-meanstd.pkl', 'rb'))\n",
        "\n",
        "    # 19 - 44 classes None\n",
        "\n",
        "    extra = 'geomfeat' if args['geomfeat'] else None\n",
        "\n",
        "    if args['preload']:\n",
        "        dt = PixelSetData_preloaded(args['dataset_folder'], labels='label_19class', npixel=args['npixel'],\n",
        "                          sub_classes= None,\n",
        "                          norm=mean_std,\n",
        "                          extra_feature=extra)\n",
        "        dt_sepT = PixelSetData_preloaded(args['dataset_folder_sepT'], labels='label_19class', npixel=args['npixel'],\n",
        "                          sub_classes= None,\n",
        "                          norm=mean_std_sepT,\n",
        "                          extra_feature=extra)        \n",
        "    else:\n",
        "        dt = PixelSetData(args['dataset_folder'], labels='label_19class', npixel=args['npixel'],\n",
        "                          sub_classes= None,\n",
        "                          norm=mean_std,\n",
        "                          extra_feature=extra)\n",
        "        dt_sepT = PixelSetData(args['dataset_folder_sepT'], labels='label_19class', npixel=args['npixel'],\n",
        "                          sub_classes= None,\n",
        "                          norm=mean_std_sepT,\n",
        "                          extra_feature=extra)        \n",
        "        \n",
        "    device = torch.device(args['device'])\n",
        "    print(\"len(dt)1: \",len(dt))\n",
        "\n",
        "    if args['separate_test'] == 'yes' :\n",
        "        loaders = get_loaders_sepT(dt, dt_sepT, args['kfold'], args)\n",
        "    elif args['separate_test'] == 'no':\n",
        "        loaders = get_loaders(dt, args['kfold'], args)\n",
        "\n",
        "    for fold, (train_loader, val_loader, test_loader) in enumerate(loaders):\n",
        "        print(\"len(dt)0: \",len(dt))\n",
        "        print(\"len(dt_sepT)0: \",len(dt_sepT))\n",
        "        print('Starting Fold {}'.format(fold + 1))\n",
        "        print('Train {}, Val {}, Test {}'.format(len(train_loader), len(val_loader), len(test_loader)))\n",
        "        print(\"test_loader: \", test_loader)\n",
        "\n",
        "        model_args= dict(input_dim=args['input_dim'], mlp1=args['mlp1'], pooling=args['pooling'],\n",
        "                            mlp2=args['mlp2'], n_head=args['n_head'], d_k=args['d_k'], mlp3=args['mlp3'],\n",
        "                            dropout=args['dropout'], T=args['T'], len_max_seq=args['lms'],\n",
        "                            positions=dt.date_positions if args['positions'] == 'bespoke' else None,\n",
        "                            mlp4=args['mlp4'])\n",
        "\n",
        "        if args['geomfeat']:\n",
        "            model_args.update(with_extra=True, extra_size=4)\n",
        "        else:\n",
        "            model_args.update(with_extra=False, extra_size=None)\n",
        "\n",
        "        model = PseTae(**model_args)\n",
        "\n",
        "        print(model.param_ratio())\n",
        "\n",
        "        model = model.to(device)\n",
        "        model.apply(weight_init)\n",
        "        optimizer = torch.optim.Adam(model.parameters())\n",
        "        criterion = FocalLoss(args['gamma'])\n",
        "\n",
        "        trainlog = {}\n",
        "\n",
        "\n",
        "\n",
        "        best_mIoU = 0\n",
        "\n",
        "        training_history = {'train_accuracy': [], 'train_loss': [], 'val_accuracy': [], 'val_loss': []} #Er\n",
        "\n",
        "        for epoch in range(1, args['epochs'] + 1):\n",
        "            print('EPOCH {}/{}'.format(epoch, args['epochs']))\n",
        "\n",
        "\n",
        "            model.train()\n",
        "            train_metrics = train_epoch(model, optimizer, criterion, train_loader, device=device, args=args)\n",
        "\n",
        "            print('Validation . . . ')\n",
        "            model.eval()\n",
        "            val_metrics = evaluation(model, criterion, val_loader, device=device, args=args, mode='val')\n",
        "\n",
        "            print('Loss {:.4f},  Acc {:.2f},  IoU {:.4f}'.format(val_metrics['val_loss'], val_metrics['val_accuracy'],\n",
        "                                                                 val_metrics['val_IoU']))\n",
        "\n",
        "            \n",
        "            trainlog[epoch] = {**train_metrics, **val_metrics}\n",
        "            checkpoint(fold + 1, trainlog, args)\n",
        "\n",
        "            training_history['train_accuracy'].append(train_metrics['train_accuracy'])           #Er\n",
        "            training_history['train_loss'].append(train_metrics['train_loss'])   #Er\n",
        "            training_history['val_accuracy'].append(val_metrics['val_accuracy'])           #Er\n",
        "            training_history['val_loss'].append(val_metrics['val_loss'])   #Er\n",
        "\n",
        "            if val_metrics['val_IoU'] >= best_mIoU:\n",
        "                best_mIoU = val_metrics['val_IoU']\n",
        "                torch.save({'epoch': epoch, 'state_dict': model.state_dict(),\n",
        "                            'optimizer': optimizer.state_dict()},\n",
        "                           os.path.join(args['res_dir'], 'Fold_{}'.format(fold + 1), 'model.pth.tar'))\n",
        "\n",
        "        print('training_history:', training_history)  #Er\n",
        "        fig, ax = plt.subplots(figsize=(20, 5))\n",
        "        ax.plot(training_history[\"train_accuracy\"], label=\"train_accuracy\")\n",
        "        ax.plot(training_history[\"train_loss\"], label=\"train_loss\")\n",
        "        ax.plot(training_history[\"val_accuracy\"], label=\"val_accuracy\")\n",
        "        ax.plot(training_history[\"val_loss\"], label=\"val_loss\")\n",
        "        ax.legend()\n",
        "        save_path = os.path.join(args['res_dir'],\"erplot_{}.png\".format(fold) )\n",
        "        plt.savefig(save_path)\n",
        "        \n",
        "        print('Testing best epoch . . .')\n",
        "        model.load_state_dict(\n",
        "            torch.load(os.path.join(args['res_dir'], 'Fold_{}'.format(fold + 1), 'model.pth.tar'))['state_dict'])\n",
        "        model.eval()\n",
        "\n",
        "        test_metrics, conf_mat = evaluation(model, criterion, test_loader, device=device, mode='test', args=args)\n",
        "\n",
        "        print('Loss {:.4f},  Acc {:.2f},  IoU {:.4f}'.format(test_metrics['test_loss'], test_metrics['test_accuracy'],\n",
        "                                                             test_metrics['test_IoU']))\n",
        "        save_results(fold + 1, test_metrics, conf_mat, args)\n",
        "\n",
        "    overall_performance(args)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aT1XanDusCP2",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Set-up parameters\n",
        "    # /home/mhbokaei/shakouri/test/Satellite/dataset_folder100\n",
        "    # /home/mhbokaei/shakouri/test/Satellite/dataset_folder10000\n",
        "    # /home/mhbokaei/shakouri/test/Satellite/dataset_folder90000\n",
        "    # /home/mhbokaei/shakouri/SatelliteImage_TimeSeries_Classification/dataset_folder\n",
        "    \n",
        "    \n",
        "    parser.add_argument('--dataset_folder', default='/home/mhbokaei/shakouri/test/Satellite/dataset_folder10000', type=str,\n",
        "                        help='Path to the folder where the results are saved.')\n",
        "    parser.add_argument('--dataset_folder_sepT', default='/home/mhbokaei/shakouri/test/Satellite/dataset_folder100', type=str,\n",
        "                        help='Path to the folder where the results are saved.')\n",
        "    parser.add_argument('--separate_test', default='no', type=str, help='Shows the dataset for Test is different')\n",
        "\n",
        "    parser.add_argument('--res_dir', default='./results', help='Path to the folder where the results should be stored')\n",
        "    parser.add_argument('--num_workers', default=8, type=int, help='Number of data loading workers')\n",
        "    parser.add_argument('--rdm_seed', default=1, type=int, help='Random seed')\n",
        "    parser.add_argument('--device', default='cuda', type=str,\n",
        "                        help='Name of device to use for tensor computations (cuda/cpu)')\n",
        "    parser.add_argument('--display_step', default=20, type=int,\n",
        "                        help='Interval in batches between display of training metrics')\n",
        "    parser.add_argument('--preload', dest='preload', action='store_true',\n",
        "                        help='If specified, the whole dataset is loaded to RAM at initialization')\n",
        "    parser.set_defaults(preload=False)\n",
        "\n",
        "    # Training parameters\n",
        "    parser.add_argument('--kfold', default=5, type=int, help='Number of folds for cross validation')\n",
        "    parser.add_argument('--epochs', default=10, type=int, help='Number of epochs per fold')\n",
        "    parser.add_argument('--batch_size', default=128, type=int, help='Batch size')\n",
        "    parser.add_argument('--lr', default=0.001, type=float, help='Learning rate')\n",
        "    parser.add_argument('--gamma', default=1, type=float, help='Gamma parameter of the focal loss')\n",
        "    parser.add_argument('--npixel', default=5, type=int, help='Number of pixels to sample from the input images')\n",
        "\n",
        "    # Architecture Hyperparameters\n",
        "    ## PSE\n",
        "    parser.add_argument('--input_dim', default=10, type=int, help='Number of channels of input images')\n",
        "    parser.add_argument('--mlp1', default='[10,32,64]', type=str, help='Number of neurons in the layers of MLP1')\n",
        "    parser.add_argument('--pooling', default='mean_std', type=str, help='Pixel-embeddings pooling strategy')\n",
        "    parser.add_argument('--mlp2', default='[132,128]', type=str, help='Number of neurons in the layers of MLP2')\n",
        "    parser.add_argument('--geomfeat', default=1, type=int,\n",
        "                        help='If 1 the precomputed geometrical features (f) are used in the PSE.')\n",
        "\n",
        "    ## TAE\n",
        "    parser.add_argument('--n_head', default=4, type=int, help='Number of attention heads')\n",
        "    parser.add_argument('--d_k', default=32, type=int, help='Dimension of the key and query vectors')\n",
        "    parser.add_argument('--mlp3', default='[512,128,128]', type=str, help='Number of neurons in the layers of MLP3')\n",
        "    parser.add_argument('--T', default=1000, type=int, help='Maximum period for the positional encoding')\n",
        "    parser.add_argument('--positions', default='bespoke', type=str,\n",
        "                        help='Positions to use for the positional encoding (bespoke / order)')\n",
        "    parser.add_argument('--lms', default=None, type=int,\n",
        "                        help='Maximum sequence length for positional encoding (only necessary if positions == order)')\n",
        "    parser.add_argument('--dropout', default=0.2, type=float, help='Dropout probability')\n",
        "\n",
        "    ## Classifier\n",
        "    parser.add_argument('--num_classes', default=19, type=int, help='Number of classes')\n",
        "    parser.add_argument('--mlp4', default='[128, 64, 32, 19]', type=str, help='Number of neurons in the layers of MLP4')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcR-4SkvsCSJ",
        "metadata": {},
        "outputId": "cb32d98d-f261-401a-8eb7-d483a6e12bc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'T': 1000,\n",
            " 'batch_size': 128,\n",
            " 'd_k': 32,\n",
            " 'dataset_folder': '/home/mhbokaei/shakouri/SatelliteImage_TimeSeries_Classification/dataset_folder',\n",
            " 'dataset_folder_sepT': '/home/mhbokaei/shakouri/test/Satellite/dataset_folder100',\n",
            " 'device': 'cuda',\n",
            " 'display_step': 20,\n",
            " 'dropout': 0.2,\n",
            " 'epochs': 20,\n",
            " 'gamma': 1,\n",
            " 'geomfeat': 1,\n",
            " 'input_dim': 10,\n",
            " 'kfold': 5,\n",
            " 'lms': None,\n",
            " 'lr': 0.001,\n",
            " 'mlp1': [10, 32, 64],\n",
            " 'mlp2': [132, 128],\n",
            " 'mlp3': [512, 128, 128],\n",
            " 'mlp4': [128, 64, 32, 19],\n",
            " 'n_head': 4,\n",
            " 'npixel': 5,\n",
            " 'num_classes': 19,\n",
            " 'num_workers': 8,\n",
            " 'pooling': 'mean_std',\n",
            " 'positions': 'bespoke',\n",
            " 'preload': False,\n",
            " 'rdm_seed': 1,\n",
            " 'res_dir': './results',\n",
            " 'separate_test': 'no'}\n",
            "len(dt)1:  191209\n",
            "len(dt)0:  191209\n",
            "len(dt_sepT)0:  98\n",
            "Starting Fold 1\n",
            "Train 897, Val 299, Test 299\n",
            "test_loader:  <torch.utils.data.dataloader.DataLoader object at 0x7f6386425880>\n",
            "TOTAL TRAINABLE PARAMETERS : 164083\n",
            "RATIOS: Spatial  12.1% , Temporal  81.1% , Classifier   6.8%\n",
            "None\n",
            "EPOCH 1/20\n",
            "Step [20/897], Loss: 2.1288, Acc : 19.53\n",
            "Step [40/897], Loss: 1.6748, Acc : 45.06\n",
            "Step [60/897], Loss: 1.3916, Acc : 57.94\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m             args[k] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, v\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m      9\u001b[0m pprint\u001b[38;5;241m.\u001b[39mpprint(args)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[4], line 262\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m    261\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m--> 262\u001b[0m train_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation . . . \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    265\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
            "Cell \u001b[0;32mIn[4], line 7\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, criterion, data_loader, device, args)\u001b[0m\n\u001b[1;32m      4\u001b[0m y_true \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[1;32m      9\u001b[0m     y_true\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, y)))\n\u001b[1;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m recursive_todevice(x, device)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1359\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1362\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1325\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1325\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1326\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1327\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1163\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
            "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/queues.py:107\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    106\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
            "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
            "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
            "File \u001b[0;32m/usr/lib/python3.8/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "args= parser.parse_args(args=[])\n",
        "args= vars(args)\n",
        "for k, v in args.items():\n",
        "        if 'mlp' in k:\n",
        "            v = v.replace('[', '')\n",
        "            v = v.replace(']', '')\n",
        "            args[k] = list(map(int, v.split(',')))\n",
        "\n",
        "pprint.pprint(args)\n",
        "main(args)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
