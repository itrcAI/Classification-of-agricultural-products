{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import torchnet as tnt\n",
        "# from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import os\n",
        "import json\n",
        "import pickle as pkl\n",
        "import argparse\n",
        "import pprint\n",
        "from datetime import datetime\n",
        "\n",
        "from models.stclassifier_fusion import PseTae\n",
        "from dataset_fusion import PixelSetData, PixelSetData_preloaded\n",
        "from learning.focal_loss import FocalLoss\n",
        "from learning.weight_init import weight_init\n",
        "from learning.metrics import mIou, confusion_matrix_analysis\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, optimizer, criterion, data_loader, device, args):\n",
        "    start = datetime.now()\n",
        "    acc_meter = tnt.meter.ClassErrorMeter(accuracy=True)\n",
        "    loss_meter = tnt.meter.AverageValueMeter()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for i, (x, x2, y, dates) in enumerate(data_loader): \n",
        "                \n",
        "        y_true.extend(list(map(int, y)))\n",
        "\n",
        "        x = recursive_todevice(x, device)\n",
        "        x2 = recursive_todevice(x2, device) \n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x, x2, dates)\n",
        "        loss = criterion(out, y.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = out.detach()\n",
        "        y_p = pred.argmax(dim=1).cpu().numpy()\n",
        "        y_pred.extend(list(y_p))\n",
        "        acc_meter.add(pred, y)\n",
        "        loss_meter.add(loss.item())\n",
        "\n",
        "        if (i + 1) % args['display_step'] == 0:\n",
        "            print('Step [{}/{}], Loss: {:.4f}, Acc : {:.2f}'.format(i + 1, len(data_loader), loss_meter.value()[0],\n",
        "                                                                    acc_meter.value()[0]))\n",
        "\n",
        "    epoch_metrics = {'train_loss': loss_meter.value()[0],\n",
        "                     'train_accuracy': acc_meter.value()[0],\n",
        "                     'train_IoU': mIou(y_true, y_pred, n_classes=args['num_classes'])}\n",
        "    print('train epoch complete in ----------------------->', datetime.now()-start)\n",
        "    return epoch_metrics\n",
        "\n",
        "\n",
        "def evaluation(model, criterion, loader, device, args, mode='val'):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    acc_meter = tnt.meter.ClassErrorMeter(accuracy=True)\n",
        "    loss_meter = tnt.meter.AverageValueMeter()\n",
        "\n",
        "    for (x, x2, y, dates) in loader: \n",
        "\n",
        "        y_true.extend(list(map(int, y)))\n",
        "\n",
        "        x = recursive_todevice(x, device)\n",
        "        x2 = recursive_todevice(x2, device) #add x2 to device\n",
        "        y = y.to(device)\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            prediction = model(x, x2, dates)  \n",
        "            loss = criterion(prediction, y)\n",
        "\n",
        "        acc_meter.add(prediction, y)\n",
        "        loss_meter.add(loss.item())\n",
        "\n",
        "        y_p = prediction.argmax(dim=1).cpu().numpy()\n",
        "        y_pred.extend(list(y_p))\n",
        "\n",
        "    metrics = {'{}_accuracy'.format(mode): acc_meter.value()[0],\n",
        "               '{}_loss'.format(mode): loss_meter.value()[0],\n",
        "               '{}_IoU'.format(mode): mIou(y_true, y_pred, args['num_classes'])}\n",
        "\n",
        "    if mode == 'val':\n",
        "        return metrics\n",
        "    elif mode == 'test':\n",
        "        return metrics, confusion_matrix(y_true, y_pred, labels=list(range(args['num_classes']))), y_true, y_pred \n",
        "\n",
        "\n",
        "def get_pse(folder, args):\n",
        "    if args['preload']:\n",
        "        dt = PixelSetData_preloaded(args[folder], labels=args['label_class'], npixel=args['npixel'],\n",
        "                          sub_classes = None,\n",
        "                          norm=None,\n",
        "                          minimum_sampling=args['minimum_sampling'],\n",
        "                          fusion_type = args['fusion_type'], interpolate_method = args['interpolate_method'],\n",
        "                          extra_feature='geomfeat' if args['geomfeat'] else None,  \n",
        "                          jitter=None)\n",
        "    else:\n",
        "        dt = PixelSetData(args[folder], labels=args['label_class'], npixel=args['npixel'],\n",
        "                          sub_classes = None,\n",
        "                          norm=None,\n",
        "                          minimum_sampling=args['minimum_sampling'],\n",
        "                          fusion_type = args['fusion_type'], interpolate_method = args['interpolate_method'],\n",
        "                          extra_feature='geomfeat' if args['geomfeat'] else None,  \n",
        "                          jitter=None)\n",
        "    return dt\n",
        "\n",
        "def get_loaders(args):\n",
        "    loader_seq =[]\n",
        "    train_dataset = get_pse('dataset_folder', args)\n",
        "    val_dataset = get_pse('val_folder', args)\n",
        "    test_dataset = get_pse('test_folder', args)\n",
        "\n",
        "    if args['dataset_folder2'] is not None:\n",
        "        train_dataset2 = get_pse('dataset_folder2', args)\n",
        "        train_dataset = data.ConcatDataset([train_dataset, train_dataset2])\n",
        "\n",
        "        \n",
        "    train_loader = data.DataLoader(train_dataset, batch_size=args['batch_size'],\n",
        "                                        num_workers=args['num_workers'], shuffle = True, pin_memory =True) \n",
        "\n",
        "    validation_loader = data.DataLoader(val_dataset, batch_size=args['batch_size'],\n",
        "                                        num_workers=args['num_workers'], shuffle = False, pin_memory = True)\n",
        "\n",
        "    test_loader = data.DataLoader(test_dataset, batch_size=args['batch_size'],\n",
        "                                    num_workers=args['num_workers'], shuffle = False, pin_memory =True)\n",
        "\n",
        "    loader_seq.append((train_loader, validation_loader, test_loader))\n",
        "    return loader_seq\n",
        "\n",
        "def recursive_todevice(x, device):\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return x.to(device)\n",
        "    else:\n",
        "        return [recursive_todevice(c, device) for c in x]\n",
        "\n",
        "\n",
        "def prepare_output(args):\n",
        "    os.makedirs(args['res_dir'], exist_ok=True)\n",
        "\n",
        "\n",
        "def checkpoint(log, args):\n",
        "    with open(os.path.join(args['res_dir'], 'trainlog.json'), 'w') as outfile:\n",
        "        json.dump(log, outfile, indent=4)\n",
        "\n",
        "\n",
        "def save_results(metrics, conf_mat, args, y_true, y_pred):\n",
        "    with open(os.path.join(args['res_dir'], 'test_metrics.json'), 'w') as outfile:\n",
        "        json.dump(metrics, outfile, indent=4)\n",
        "    pkl.dump(conf_mat, open(os.path.join(args['res_dir'], 'conf_mat.pkl'), 'wb'))\n",
        "\n",
        "\n",
        "    # save y_true, y_pred\n",
        "    pkl.dump(y_true, open(os.path.join(args['res_dir'], 'y_true_test_data.pkl'), 'wb'))\n",
        "    pkl.dump(y_pred, open(os.path.join(args['res_dir'], 'y_pred_test_data.pkl'), 'wb'))\n",
        "\n",
        "    # ----> save confusion matrix \n",
        "    plt.figure(figsize=(15,10))\n",
        "    img = sns.heatmap(conf_mat, annot = True, fmt='d',linewidths=0.5, cmap='OrRd')\n",
        "    img.tick_params(top=True, labeltop=True, bottom=False, labelbottom=False)\n",
        "    img.set(ylabel=\"True Label\", xlabel=\"Predicted Label\")\n",
        "    img.figure.savefig(os.path.join(args['res_dir'], 'conf_mat_picture.png'))\n",
        "    img.get_figure().clf()\n",
        "\n",
        "\n",
        "def overall_performance(args):\n",
        "    cm = np.zeros((args['num_classes'], args['num_classes']))\n",
        "    cm += pkl.load(open(os.path.join(args['res_dir'], 'conf_mat.pkl'), 'rb'))\n",
        "    per_class, perf = confusion_matrix_analysis(cm)\n",
        "\n",
        "    print('Overall performance:')\n",
        "    print('Acc: {},  IoU: {}'.format(perf['Accuracy'], perf['MACRO_IoU']))\n",
        "\n",
        "    with open(os.path.join(args['res_dir'], 'overall.json'), 'w') as file:\n",
        "        file.write(json.dumps(perf, indent=4))\n",
        "    with open(os.path.join(args['res_dir'], 'per_class.json'), 'w') as file:\n",
        "        file.write(json.dumps(per_class, indent=4))\n",
        "\n",
        "\n",
        "########Number of classes#######er\n",
        "def point_plot(data,\n",
        "               D_list,  ##List of deleted classes\n",
        "               path : str,\n",
        "               x_lable : str,\n",
        "               y_lable : str,\n",
        "               title : str):\n",
        "    \n",
        "    for i in D_list:\n",
        "     del data[i]\n",
        "    \n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    x = list(data.keys())\n",
        "    y = list(data.values())\n",
        "    x = list(map(lambda x : str(x), x))\n",
        "\n",
        "    for _x, _y in zip(x, y):\n",
        "        plt.text(_x, _y, f'{_y:.0f}', fontsize=9, ha='center', va='bottom')\n",
        "    plt.plot(x, y, marker='o', linestyle='--')\n",
        "    plt.xlabel(x_lable)\n",
        "    plt.ylabel(y_lable)\n",
        "    plt.title(title)\n",
        "    plt.grid(True)\n",
        "    plt.savefig(path)\n",
        "    print(\"data.keys():\", data.keys())\n",
        "    print(\"len(data.keys()):\", len(data.keys()))\n",
        "    #plt.show()\n",
        "    plt.close()\n",
        "\n",
        "### for Train\n",
        "def Data_distribution_train(args):\n",
        "\n",
        "    data_folder = os.path.join(args['dataset_folder'] , 'DATA')\n",
        "    l = [f for f in os.listdir(data_folder) if f.endswith('.npy')]\n",
        "    pid = [int(f.split('.')[0]) for f in l]\n",
        "    pid = list(np.sort(pid))\n",
        "\n",
        "    with open(os.path.join(args['dataset_folder'], 'META', 'labels.json'), 'r') as file:\n",
        "        data = json.load(file)\n",
        "    Dic = data[args['label_class']]\n",
        "    converted_Dic = {int(key): value for key, value in Dic.items()}\n",
        "    Final_dic = {key: converted_Dic[key] for key in pid if key in converted_Dic}\n",
        "\n",
        "    class_19_44 = list(Final_dic.values())\n",
        "    counter = {}\n",
        "    for _class in class_19_44:\n",
        "        if _class in counter:\n",
        "            counter[_class] += 1\n",
        "        elif _class not in counter:\n",
        "            counter[_class] = 0\n",
        "    counter = dict(sorted(counter.items(), key=lambda item:item[0]))\n",
        "    save_path_cn = os.path.join(args['res_dir'], \"number_of_Trainclasses.png\")\n",
        "    point_plot(counter,args['Delet_label_class'], save_path_cn, \"Classes\", \"Number\", \"Number of each class\")\n",
        "\n",
        "###For Validations data\n",
        "def Data_distribution_val(args):\n",
        "\n",
        "    data_folder = os.path.join(args['val_folder'] , 'DATA')\n",
        "    l = [f for f in os.listdir(data_folder) if f.endswith('.npy')]\n",
        "    pid = [int(f.split('.')[0]) for f in l]\n",
        "    pid = list(np.sort(pid))\n",
        "\n",
        "    with open(os.path.join(args['val_folder'], 'META', 'labels.json'), 'r') as file:\n",
        "        data = json.load(file)\n",
        "    Dic = data[args['label_class']]\n",
        "    converted_Dic = {int(key): value for key, value in Dic.items()}\n",
        "    Final_dic = {key: converted_Dic[key] for key in pid if key in converted_Dic}\n",
        "\n",
        "    class_19_44 = list(Final_dic.values())\n",
        "    counter = {}\n",
        "    for _class in class_19_44:\n",
        "        if _class in counter:\n",
        "            counter[_class] += 1\n",
        "        elif _class not in counter:\n",
        "            counter[_class] = 0\n",
        "    counter = dict(sorted(counter.items(), key=lambda item:item[0]))\n",
        "    save_path_cn = os.path.join(args['res_dir'], \"number_of_valclasses.png\")\n",
        "    point_plot(counter,args['Delet_label_class'], save_path_cn, \"Classes\", \"Number\", \"Number of each class\")\n",
        "\n",
        "## for Test data\n",
        "def Data_distribution_test(args):\n",
        "\n",
        "    data_folder = os.path.join(args['test_folder'] , 'DATA')\n",
        "    l = [f for f in os.listdir(data_folder) if f.endswith('.npy')]\n",
        "    pid = [int(f.split('.')[0]) for f in l]\n",
        "    pid = list(np.sort(pid))\n",
        "\n",
        "    with open(os.path.join(args['test_folder'], 'META', 'labels.json'), 'r') as file:\n",
        "        data = json.load(file)\n",
        "    Dic = data[args['label_class']]\n",
        "    converted_Dic = {int(key): value for key, value in Dic.items()}\n",
        "    Final_dic = {key: converted_Dic[key] for key in pid if key in converted_Dic}\n",
        "\n",
        "    class_19_44 = list(Final_dic.values())\n",
        "    counter = {}\n",
        "    for _class in class_19_44:\n",
        "        if _class in counter:\n",
        "            counter[_class] += 1\n",
        "        elif _class not in counter:\n",
        "            counter[_class] = 0\n",
        "    counter = dict(sorted(counter.items(), key=lambda item:item[0]))\n",
        "    save_path_cn = os.path.join(args['res_dir'], \"number_of_testclasses.png\")\n",
        "    point_plot(counter,args['Delet_label_class'], save_path_cn, \"Classes\", \"Number\", \"Number of each class\")\n",
        "\n",
        "###########################################################################################\n",
        "\n",
        "\n",
        "def plot_metrics(args):\n",
        "    with open(os.path.join(args['res_dir'], 'trainlog.json'), 'r') as file:\n",
        "        d = json.loads(file.read())\n",
        "    \n",
        "    epoch = [i+1 for i in range(len(d))]\n",
        "    train_loss = [d[str(i+1)][\"train_loss\"] for i in range(len(d))]\n",
        "    val_loss = [d[str(i+1)][\"val_loss\"] for i in range(len(d))]\n",
        "    train_acc = [d[str(i+1)][\"train_accuracy\"] for i in range(len(d))]\n",
        "    val_acc = [d[str(i+1)][\"val_accuracy\"] for i in range(len(d))]\n",
        "\n",
        "    # plot loss/accuracy  #########er\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))  # Adjust figsize as needed\n",
        "    ax1.plot(epoch,train_acc, label=\"train_accuracy\", lw = 3, linestyle ='-')\n",
        "    ax1.plot(epoch, val_acc, label=\"val_accuracy\", lw = 3, linestyle ='--')\n",
        "    ax1.set_ylabel('Accuracy (%)', fontsize=15)\n",
        "    ax1.set_xlabel('Epoch', fontsize=15)\n",
        "    ax1.legend()\n",
        "    ax1.tick_params(axis='both', which='major', labelsize=15)\n",
        "    ax2.plot(epoch,train_loss, label=\"train_loss\", lw = 3, linestyle ='-')\n",
        "    ax2.plot(epoch,val_loss, label=\"val_loss\", lw = 3, linestyle ='--')\n",
        "    ax2.set_ylabel('Loss ', fontsize=15)\n",
        "    ax2.set_xlabel('Epoch', fontsize=15)\n",
        "    ax2.legend()\n",
        "    ax2.tick_params(axis='both', which='major', labelsize=15)\n",
        "    plt.tight_layout()\n",
        "    save_path = os.path.join(args['res_dir'],\"evplot.png\")\n",
        "    plt.savefig(save_path)\n",
        "    plt.clf()\n",
        "\n",
        "    #---------------------------------------------\n",
        "\n",
        "def main(args):\n",
        "    np.random.seed(args['rdm_seed'])\n",
        "    torch.manual_seed(args['rdm_seed'])\n",
        "    prepare_output(args)\n",
        "\n",
        "    extra = 'geomfeat' if args['geomfeat'] else None\n",
        "\n",
        "    device = torch.device(args['device'])\n",
        "\n",
        "    Data_distribution_train(args)\n",
        "    Data_distribution_val(args)\n",
        "    Data_distribution_test(args)\n",
        "\n",
        "    loaders = get_loaders(args)\n",
        "    for _, (train_loader, val_loader, test_loader) in enumerate(loaders):\n",
        "        print('Train {}, Val {}, Test {}'.format(len(train_loader), len(val_loader), len(test_loader)))\n",
        "\n",
        "        model_args = dict(input_dim=args['input_dim'], mlp1=args['mlp1'], pooling=args['pooling'],\n",
        "                            mlp2=args['mlp2'], n_head=args['n_head'], d_k=args['d_k'], mlp3=args['mlp3'],\n",
        "                            dropout=args['dropout'], T=args['T'], len_max_seq=args['lms'],\n",
        "                            positions=None, fusion_type = args['fusion_type'],\n",
        "                            mlp4=args['mlp4'])\n",
        "\n",
        "        if args['geomfeat']:\n",
        "            model_args.update(with_extra=True, extra_size=4) \n",
        "        else:\n",
        "            model_args.update(with_extra=False, extra_size=None)\n",
        "\n",
        "        model = PseTae(**model_args)\n",
        "\n",
        "        print(model.param_ratio())\n",
        "\n",
        "\n",
        "        model = model.to(device)\n",
        "        model.apply(weight_init)\n",
        "        optimizer = torch.optim.Adam(model.parameters())\n",
        "        criterion = FocalLoss(args['gamma'])\n",
        "\n",
        "        trainlog = {}\n",
        "\n",
        "\n",
        "        best_mIoU = 0\n",
        "        for epoch in range(1, args['epochs'] + 1):\n",
        "            print('EPOCH {}/{}'.format(epoch, args['epochs']))\n",
        "\n",
        "            model.train()\n",
        "            train_metrics = train_epoch(model, optimizer, criterion, train_loader, device=device, args=args)\n",
        "\n",
        "            print('Validation . . . ')\n",
        "            model.eval()\n",
        "            val_metrics = evaluation(model, criterion, val_loader, device=device, args=args, mode='val')\n",
        "\n",
        "            print('Loss {:.4f},  Acc {:.2f},  IoU {:.4f}'.format(val_metrics['val_loss'], val_metrics['val_accuracy'],\n",
        "                                                                 val_metrics['val_IoU']))\n",
        "\n",
        "            trainlog[epoch] = {**train_metrics, **val_metrics}\n",
        "            checkpoint(trainlog, args)\n",
        "\n",
        "            if val_metrics['val_IoU'] >= best_mIoU:\n",
        "                best_mIoU = val_metrics['val_IoU']\n",
        "                torch.save({'epoch': epoch, 'state_dict': model.state_dict(),\n",
        "                            'optimizer': optimizer.state_dict()},\n",
        "                           os.path.join(args['res_dir'], 'model.pth.tar'))\n",
        "\n",
        "        print('Testing best epoch . . .')\n",
        "        model.load_state_dict(\n",
        "            torch.load(os.path.join(args['res_dir'],  'model.pth.tar'))['state_dict'])\n",
        "        model.eval()\n",
        "\n",
        "        test_metrics, conf_mat, y_true, y_pred = evaluation(model, criterion, test_loader, device=device, mode='test', args=args) \n",
        "\n",
        "        print('Loss {:.4f},  Acc {:.2f},  IoU {:.4f}'.format(test_metrics['test_loss'], test_metrics['test_accuracy'],\n",
        "                                                             test_metrics['test_IoU']))\n",
        "                                                             \n",
        "        save_results(test_metrics, conf_mat, args, y_true, y_pred) \n",
        "\n",
        "    overall_performance(args)\n",
        "    plot_metrics(args)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Delet_label_class': [],\n",
            " 'T': 1000,\n",
            " 'batch_size': 30,\n",
            " 'd_k': 32,\n",
            " 'dataset_folder': '/home/mhbokaei/shakouri/CropTypeMappinp/multi_sensor/All_dataset/dataset_10000/dataset_folder/s1_data',\n",
            " 'dataset_folder2': None,\n",
            " 'device': 'cuda',\n",
            " 'display_step': 50,\n",
            " 'dropout': 0.2,\n",
            " 'epochs': 20,\n",
            " 'fusion_type': 'tsa',\n",
            " 'gamma': 1,\n",
            " 'geomfeat': 1,\n",
            " 'input_dim': 10,\n",
            " 'interpolate_method': 'nn',\n",
            " 'label_class': 'label_19class',\n",
            " 'lms': None,\n",
            " 'lr': 0.001,\n",
            " 'minimum_sampling': None,\n",
            " 'mlp1': [10, 32, 64],\n",
            " 'mlp2': [132, 128],\n",
            " 'mlp3': [512, 128, 128],\n",
            " 'mlp4': [256, 64, 32, 19],\n",
            " 'n_head': 4,\n",
            " 'npixel': 64,\n",
            " 'num_classes': 19,\n",
            " 'num_workers': 8,\n",
            " 'pooling': 'mean_std',\n",
            " 'positions': 'bespoke',\n",
            " 'preload': False,\n",
            " 'rdm_seed': 1,\n",
            " 'res_dir': './results',\n",
            " 'test_folder': '/home/mhbokaei/shakouri/CropTypeMappinp/multi_sensor/All_dataset/dataset_10000/test_folder/s1_data',\n",
            " 'val_folder': '/home/mhbokaei/shakouri/CropTypeMappinp/multi_sensor/All_dataset/dataset_10000/val_folder/s1_data'}\n",
            "data.keys(): dict_keys([0, 1, 2, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18])\n",
            "len(data.keys()): 16\n",
            "data.keys(): dict_keys([1, 2, 5, 6, 7, 8, 10, 11, 13, 14, 15, 17, 18])\n",
            "len(data.keys()): 13\n",
            "data.keys(): dict_keys([1, 2, 5, 6, 7, 8, 10, 11, 13, 15])\n",
            "len(data.keys()): 10\n",
            "Train 200, Val 67, Test 67\n",
            "TOTAL TRAINABLE PARAMETERS : 324947\n",
            "RATIOS: Spatial  12.2% , Temporal  81.9% , Classifier   6.0%\n",
            "None\n",
            "EPOCH 1/20\n",
            "Step [50/200], Loss: 1.9324, Acc : 44.87\n",
            "Step [100/200], Loss: 1.3903, Acc : 64.20\n",
            "Step [150/200], Loss: 1.1145, Acc : 71.91\n",
            "Step [200/200], Loss: 0.9438, Acc : 76.09\n",
            "train epoch complete in -----------------------> 0:00:05.053819\n",
            "Validation . . . \n",
            "Loss 0.3016,  Acc 91.88,  IoU 0.1821\n",
            "EPOCH 2/20\n",
            "Step [50/200], Loss: 0.3362, Acc : 90.00\n",
            "Step [100/200], Loss: 0.3326, Acc : 90.20\n",
            "Step [150/200], Loss: 0.3248, Acc : 90.18\n",
            "Step [200/200], Loss: 0.3267, Acc : 90.06\n",
            "train epoch complete in -----------------------> 0:00:04.043030\n",
            "Validation . . . \n",
            "Loss 0.2198,  Acc 91.78,  IoU 0.2057\n",
            "EPOCH 3/20\n",
            "Step [50/200], Loss: 0.2504, Acc : 91.80\n",
            "Step [100/200], Loss: 0.2680, Acc : 90.83\n",
            "Step [150/200], Loss: 0.2632, Acc : 91.00\n",
            "Step [200/200], Loss: 0.2539, Acc : 91.08\n",
            "train epoch complete in -----------------------> 0:00:04.030994\n",
            "Validation . . . \n",
            "Loss 0.1865,  Acc 92.83,  IoU 0.2489\n",
            "EPOCH 4/20\n",
            "Step [50/200], Loss: 0.2450, Acc : 91.47\n",
            "Step [100/200], Loss: 0.2365, Acc : 91.17\n",
            "Step [150/200], Loss: 0.2280, Acc : 91.60\n",
            "Step [200/200], Loss: 0.2336, Acc : 91.40\n",
            "train epoch complete in -----------------------> 0:00:04.005597\n",
            "Validation . . . \n",
            "Loss 0.2551,  Acc 88.77,  IoU 0.1868\n",
            "EPOCH 5/20\n",
            "Step [50/200], Loss: 0.2357, Acc : 91.27\n",
            "Step [100/200], Loss: 0.2224, Acc : 91.50\n",
            "Step [150/200], Loss: 0.2215, Acc : 91.56\n",
            "Step [200/200], Loss: 0.2149, Acc : 91.90\n",
            "train epoch complete in -----------------------> 0:00:04.000412\n",
            "Validation . . . \n",
            "Loss 0.1704,  Acc 93.68,  IoU 0.2645\n",
            "EPOCH 6/20\n",
            "Step [50/200], Loss: 0.1868, Acc : 93.07\n",
            "Step [100/200], Loss: 0.1898, Acc : 92.63\n",
            "Step [150/200], Loss: 0.1934, Acc : 92.49\n",
            "Step [200/200], Loss: 0.1914, Acc : 92.72\n",
            "train epoch complete in -----------------------> 0:00:04.002851\n",
            "Validation . . . \n",
            "Loss 0.1497,  Acc 93.68,  IoU 0.2792\n",
            "EPOCH 7/20\n",
            "Step [50/200], Loss: 0.1972, Acc : 92.47\n",
            "Step [100/200], Loss: 0.1866, Acc : 92.47\n",
            "Step [150/200], Loss: 0.1796, Acc : 92.73\n",
            "Step [200/200], Loss: 0.1818, Acc : 92.51\n",
            "train epoch complete in -----------------------> 0:00:03.993795\n",
            "Validation . . . \n",
            "Loss 0.1486,  Acc 93.43,  IoU 0.3034\n",
            "EPOCH 8/20\n",
            "Step [50/200], Loss: 0.1548, Acc : 94.07\n",
            "Step [100/200], Loss: 0.1654, Acc : 93.27\n",
            "Step [150/200], Loss: 0.1665, Acc : 93.11\n",
            "Step [200/200], Loss: 0.1666, Acc : 93.08\n",
            "train epoch complete in -----------------------> 0:00:04.067971\n",
            "Validation . . . \n",
            "Loss 0.1608,  Acc 94.09,  IoU 0.3020\n",
            "EPOCH 9/20\n",
            "Step [50/200], Loss: 0.1776, Acc : 92.47\n",
            "Step [100/200], Loss: 0.1719, Acc : 92.30\n",
            "Step [150/200], Loss: 0.1642, Acc : 92.82\n",
            "Step [200/200], Loss: 0.1575, Acc : 93.07\n",
            "train epoch complete in -----------------------> 0:00:04.085849\n",
            "Validation . . . \n",
            "Loss 0.1648,  Acc 92.53,  IoU 0.3400\n",
            "EPOCH 10/20\n",
            "Step [50/200], Loss: 0.1351, Acc : 94.13\n",
            "Step [100/200], Loss: 0.1615, Acc : 93.10\n",
            "Step [150/200], Loss: 0.1668, Acc : 93.00\n",
            "Step [200/200], Loss: 0.1612, Acc : 93.28\n",
            "train epoch complete in -----------------------> 0:00:04.021623\n",
            "Validation . . . \n",
            "Loss 0.1385,  Acc 94.19,  IoU 0.3454\n",
            "EPOCH 11/20\n",
            "Step [50/200], Loss: 0.1410, Acc : 93.67\n",
            "Step [100/200], Loss: 0.1434, Acc : 93.60\n",
            "Step [150/200], Loss: 0.1480, Acc : 93.64\n",
            "Step [200/200], Loss: 0.1513, Acc : 93.70\n",
            "train epoch complete in -----------------------> 0:00:04.046835\n",
            "Validation . . . \n",
            "Loss 0.1360,  Acc 94.29,  IoU 0.2866\n",
            "EPOCH 12/20\n",
            "Step [50/200], Loss: 0.1523, Acc : 93.27\n",
            "Step [100/200], Loss: 0.1465, Acc : 93.33\n",
            "Step [150/200], Loss: 0.1416, Acc : 93.71\n",
            "Step [200/200], Loss: 0.1447, Acc : 93.63\n",
            "train epoch complete in -----------------------> 0:00:04.107077\n",
            "Validation . . . \n",
            "Loss 0.1551,  Acc 92.38,  IoU 0.3156\n",
            "EPOCH 13/20\n",
            "Step [50/200], Loss: 0.1488, Acc : 93.27\n",
            "Step [100/200], Loss: 0.1358, Acc : 93.73\n",
            "Step [150/200], Loss: 0.1423, Acc : 93.40\n",
            "Step [200/200], Loss: 0.1463, Acc : 93.27\n",
            "train epoch complete in -----------------------> 0:00:04.007545\n",
            "Validation . . . \n",
            "Loss 0.1420,  Acc 94.79,  IoU 0.3842\n",
            "EPOCH 14/20\n",
            "Step [50/200], Loss: 0.1362, Acc : 94.73\n",
            "Step [100/200], Loss: 0.1230, Acc : 94.83\n",
            "Step [150/200], Loss: 0.1255, Acc : 94.36\n",
            "Step [200/200], Loss: 0.1297, Acc : 94.09\n",
            "train epoch complete in -----------------------> 0:00:04.065911\n",
            "Validation . . . \n",
            "Loss 0.1232,  Acc 94.99,  IoU 0.3750\n",
            "EPOCH 15/20\n",
            "Step [50/200], Loss: 0.1220, Acc : 93.93\n",
            "Step [100/200], Loss: 0.1212, Acc : 94.27\n",
            "Step [150/200], Loss: 0.1181, Acc : 94.29\n",
            "Step [200/200], Loss: 0.1200, Acc : 94.19\n",
            "train epoch complete in -----------------------> 0:00:04.020181\n",
            "Validation . . . \n",
            "Loss 0.1210,  Acc 93.58,  IoU 0.4004\n",
            "EPOCH 16/20\n",
            "Step [50/200], Loss: 0.1071, Acc : 94.40\n",
            "Step [100/200], Loss: 0.1175, Acc : 93.93\n",
            "Step [150/200], Loss: 0.1236, Acc : 94.04\n",
            "Step [200/200], Loss: 0.1192, Acc : 94.32\n",
            "train epoch complete in -----------------------> 0:00:04.038093\n",
            "Validation . . . \n",
            "Loss 0.1165,  Acc 94.64,  IoU 0.3667\n",
            "EPOCH 17/20\n",
            "Step [50/200], Loss: 0.1321, Acc : 93.60\n",
            "Step [100/200], Loss: 0.1256, Acc : 94.03\n",
            "Step [150/200], Loss: 0.1297, Acc : 93.78\n",
            "Step [200/200], Loss: 0.1242, Acc : 94.04\n",
            "train epoch complete in -----------------------> 0:00:04.041077\n",
            "Validation . . . \n",
            "Loss 0.1273,  Acc 94.04,  IoU 0.3537\n",
            "EPOCH 18/20\n",
            "Step [50/200], Loss: 0.1360, Acc : 94.33\n",
            "Step [100/200], Loss: 0.1243, Acc : 94.20\n",
            "Step [150/200], Loss: 0.1292, Acc : 94.04\n",
            "Step [200/200], Loss: 0.1312, Acc : 93.97\n",
            "train epoch complete in -----------------------> 0:00:04.028816\n",
            "Validation . . . \n",
            "Loss 0.1570,  Acc 91.88,  IoU 0.3287\n",
            "EPOCH 19/20\n",
            "Step [50/200], Loss: 0.1246, Acc : 94.40\n",
            "Step [100/200], Loss: 0.1112, Acc : 95.07\n",
            "Step [150/200], Loss: 0.1124, Acc : 94.87\n",
            "Step [200/200], Loss: 0.1179, Acc : 94.59\n",
            "train epoch complete in -----------------------> 0:00:04.051797\n",
            "Validation . . . \n",
            "Loss 0.1373,  Acc 93.48,  IoU 0.3366\n",
            "EPOCH 20/20\n",
            "Step [50/200], Loss: 0.1078, Acc : 94.20\n",
            "Step [100/200], Loss: 0.1112, Acc : 94.53\n",
            "Step [150/200], Loss: 0.1101, Acc : 94.47\n",
            "Step [200/200], Loss: 0.1105, Acc : 94.49\n",
            "train epoch complete in -----------------------> 0:00:04.036076\n",
            "Validation . . . \n",
            "Loss 0.1334,  Acc 94.29,  IoU 0.4007\n",
            "Testing best epoch . . .\n",
            "Loss 0.1389,  Acc 94.59,  IoU 0.5157\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mhbokaei/shakouri/CropTypeMappinp/multi_sensor/learning/metrics.py:64: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  d['IoU'] = tp / (tp + fp + fn)\n",
            "/home/mhbokaei/shakouri/CropTypeMappinp/multi_sensor/learning/metrics.py:65: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  d['Precision'] = tp / (tp + fp)\n",
            "/home/mhbokaei/shakouri/CropTypeMappinp/multi_sensor/learning/metrics.py:66: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  d['Recall'] = tp / (tp + fn)\n",
            "/home/mhbokaei/shakouri/CropTypeMappinp/multi_sensor/learning/metrics.py:67: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  d['F1-score'] = 2 * tp / (2 * tp + fp + fn)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overall performance:\n",
            "Acc: 0.9458646616541353,  IoU: 0.5156681578950569\n",
            "total elapsed time is ---> 0:02:20.460960\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1500x1000 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1500x700 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "    start = datetime.now()\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    #/home/mhbokaei/shakouri/CropTypeMappinp/multi_sensor/All_dataset/dataset_100/dataset_folder/s1_data\n",
        "    #/home/mhbokaei/shakouri/CropTypeMappinp/multi_sensor/All_dataset/dataset_100/test_folder/s1_data\n",
        "    #/home/mhbokaei/shakouri/CropTypeMappinp/multi_sensor/All_dataset/dataset_100/val_folder/s1_data\n",
        "    \n",
        "\n",
        "\n",
        "    # Set-up parameters\n",
        "    parser.add_argument('--dataset_folder', default='/home/mhbokaei/shakouri/CropTypeMappinp/multi_sensor/All_dataset/dataset_10000/dataset_folder/s1_data', type=str,\n",
        "                        help='Path to the folder where the results are saved.')\n",
        "\n",
        "    # set-up data loader folders -----------------------------\n",
        "    parser.add_argument('--dataset_folder2', default=None, type=str,\n",
        "                        help='Path to second train folder to concat with first initial loader.')\n",
        "    parser.add_argument('--val_folder', default='/home/mhbokaei/shakouri/CropTypeMappinp/multi_sensor/All_dataset/dataset_10000/val_folder/s1_data', type=str,\n",
        "                        help='Path to the validation folder.')\n",
        "    parser.add_argument('--test_folder', default='/home/mhbokaei/shakouri/CropTypeMappinp/multi_sensor/All_dataset/dataset_10000/test_folder/s1_data', type=str,\n",
        "                        help='Path to the test folder.')\n",
        "\n",
        "    # ---------------------------add sensor argument to test s1/s2\n",
        "    parser.add_argument('--minimum_sampling', default=None, type=int,\n",
        "                        help='minimum time series length to sample')      \n",
        "    parser.add_argument('--fusion_type', default='tsa', type=str,\n",
        "                        help='level of multi-sensor fusion e.g. early, pse, tsa, softmax_avg, softmax_norm')\n",
        "    parser.add_argument('--interpolate_method', default='nn', type=str,\n",
        "                        help='type of interpolation for early and pse fusion. eg. \"nn\",\"linear\"')    \n",
        "    \n",
        "    parser.add_argument('--res_dir', default='./results', help='Path to the folder where the results should be stored')\n",
        "    parser.add_argument('--num_workers', default=8, type=int, help='Number of data loading workers')\n",
        "    parser.add_argument('--rdm_seed', default=1, type=int, help='Random seed')\n",
        "    parser.add_argument('--device', default='cuda', type=str,\n",
        "                        help='Name of device to use for tensor computations (cuda/cpu)')\n",
        "    parser.add_argument('--display_step', default=50, type=int,\n",
        "                        help='Interval in batches between display of training metrics')\n",
        "    parser.add_argument('--preload', dest='preload', action='store_true',\n",
        "                        help='If specified, the whole dataset is loaded to RAM at initialization')\n",
        "    parser.set_defaults(preload=False)\n",
        "    parser.add_argument('--label_class', default='label_19class', type=str, help='it can be label_19class or label_44class')\n",
        "    parser.add_argument('--Delet_label_class', default=[], type=list, help='it can be label_19class or label_44class')\n",
        "\n",
        "    # Training parameters\n",
        "    parser.add_argument('--epochs', default=20, type=int, help='Number of epochs per fold')\n",
        "    parser.add_argument('--batch_size', default=30\n",
        "                        , type=int, help='Batch size')\n",
        "    parser.add_argument('--lr', default=0.001, type=float, help='Learning rate')\n",
        "    parser.add_argument('--gamma', default=1, type=float, help='Gamma parameter of the focal loss')\n",
        "    parser.add_argument('--npixel', default=64, type=int, help='Number of pixels to sample from the input images')\n",
        "\n",
        "    # Architecture Hyperparameters\n",
        "    ## PSE\n",
        "    parser.add_argument('--input_dim', default=10, type=int, help='Number of channels of input images')\n",
        "    parser.add_argument('--mlp1', default='[10,32,64]', type=str, help='Number of neurons in the layers of MLP1')\n",
        "    parser.add_argument('--pooling', default='mean_std', type=str, help='Pixel-embeddings pooling strategy')\n",
        "    parser.add_argument('--mlp2', default='[132,128]', type=str, help='Number of neurons in the layers of MLP2')\n",
        "    parser.add_argument('--geomfeat', default=1, type=int,\n",
        "                        help='If 1 the precomputed geometrical features (f) are used in the PSE.')\n",
        "\n",
        "    ## TAE\n",
        "    parser.add_argument('--n_head', default=4, type=int, help='Number of attention heads')\n",
        "    parser.add_argument('--d_k', default=32, type=int, help='Dimension of the key and query vectors')\n",
        "    parser.add_argument('--mlp3', default='[512,128,128]', type=str, help='Number of neurons in the layers of MLP3')\n",
        "    parser.add_argument('--T', default=1000, type=int, help='Maximum period for the positional encoding')\n",
        "    parser.add_argument('--positions', default='bespoke', type=str,\n",
        "                        help='Positions to use for the positional encoding (bespoke / order)')\n",
        "    parser.add_argument('--lms', default=None, type=int,\n",
        "                        help='Maximum sequence length for positional encoding (only necessary if positions == order)')\n",
        "    parser.add_argument('--dropout', default=0.2, type=float, help='Dropout probability')\n",
        "\n",
        "    ## Classifier\n",
        "    parser.add_argument('--num_classes', default=19, type=int, help='Number of classes')\n",
        "    parser.add_argument('--mlp4', default='[256, 64, 32, 19]', type=str, help='Number of neurons in the layers of MLP4- pse and tae nedd 256 except 128')\n",
        "\n",
        "    args= parser.parse_args(args=[])\n",
        "    args= vars(args)\n",
        "    for k, v in args.items():\n",
        "            if 'mlp' in k:\n",
        "                v = v.replace('[', '')\n",
        "                v = v.replace(']', '')\n",
        "                args[k] = list(map(int, v.split(',')))\n",
        "\n",
        "    pprint.pprint(args)\n",
        "    main(args)\n",
        "\n",
        "\n",
        "    #add processing time\n",
        "    print('total elapsed time is --->', datetime.now() -start)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
